# Parallel LLM Evaluation Example

This example demonstrates **safe concurrent execution** of multiple LLM evaluation sessions while properly handling MLX's thread safety requirements.

## ğŸ¯ Purpose

This example addresses the key challenge of running multiple LLM sessions simultaneously (like a server responding to multiple requests) while ensuring thread safety. It demonstrates:

- âœ… **Concurrent session execution** - Multiple LLM evaluations running in parallel
- âœ… **Thread safety** - Proper handling of unevaluated MLXArrays
- âœ… **Isolated random states** - Task-local random state using `withRandomState`
- âœ… **Session isolation** - Each session gets its own KV cache
- âœ… **GPU serialization awareness** - Understanding MLX's GPU operation model

## ğŸš€ Key Features

### Thread Safety Solutions

1. **Pre-evaluated Model Weights**
   ```swift
   // CRITICAL: Force evaluation of all parameters
   // This ensures no unevaluated MLXArrays remain
   eval(context.model)
   ```

2. **Task-Local Random State**
   ```swift
   // Each session gets its own isolated random state
   let sessionRandomState = MLXRandom.RandomState(seed: UInt64(sessionId * 1000))

   try withRandomState(sessionRandomState) {
       // Safe concurrent random operations
   }
   ```

3. **Session-Isolated KV Caches**
   - Each evaluation session gets its own `TokenIterator`
   - No shared KV cache state between sessions
   - Automatic cache management per session

### Performance Characteristics

- **CPU Parallelism**: Preprocessing and tokenization can run concurrently
- **GPU Serialization**: MLX operations serialize on GPU but maintain thread safety
- **Memory Management**: Configurable GPU cache limits (`20MB` default)

## ğŸ“Š Usage

### Basic Usage

```swift
let example = ParallelEvaluationExample()
try await example.runDefaultExample()
```

### Advanced Configuration

```swift
let config = ParallelEvaluationExample.ParallelConfig(
    numConcurrentSessions: 3,
    modelConfiguration: LLMRegistry.qwen3_1_7b_4bit,
    prompts: [
        "Write a haiku about AI.",
        "Explain neural networks simply.",
        "What is machine learning?"
    ],
    maxTokens: 100,
    temperature: 0.7
)

try await example.runParallelEvaluation(config: config)
```

### Integration with LLMEval App

The example includes a **"Run Parallel Demo"** button in the main LLMEval app that demonstrates the parallel execution. This shows how to integrate concurrent evaluation into existing applications.

## ğŸ”§ Technical Implementation

### Core Components

1. **`ParallelEvaluationExample`** - Main demonstration class
2. **`ParallelConfig`** - Configuration for concurrent sessions
3. **`EvaluationResult`** - Structured results from each session
4. **Task-based concurrency** - Swift's structured concurrency for safe parallel execution

### Thread Safety Mechanisms

| Concern | Solution | Implementation |
|---------|----------|----------------|
| Unevaluated MLXArrays | Pre-evaluation | `eval(context.model)` |
| Global random state | Task-local state | `withRandomState(randomState)` |
| Shared KV cache | Session isolation | Per-session `TokenIterator` |
| GPU operations | Serialization awareness | MLX's built-in GPU management |

### Performance Monitoring

Each session tracks:
- **Tokens per second** - Generation throughput
- **Total tokens** - Output length
- **Duration** - Session execution time
- **Session ID** - For result correlation

## ğŸ¯ Example Output

```
ğŸ¯ Starting parallel LLM evaluation with 3 sessions
ğŸ“‹ Model: mlx-community/Qwen3-1.7B-4bit
ğŸ”¥ Temperature: 0.7
ğŸ“ Max tokens: 100

âš¡ Running 3 evaluation sessions concurrently...
ğŸ’¡ Note: GPU operations will serialize, but CPU preprocessing can run in parallel

ğŸš€ Starting session 0 with prompt: Write a short poem about machine learning...
ğŸš€ Starting session 1 with prompt: Explain the concept of neural networks simply...
ğŸš€ Starting session 2 with prompt: What are the key differences between AI and machine learning?...

âœ… Session 0 completed: 45.23 tokens/sec, 98 tokens
âœ… Session 1 completed: 42.15 tokens/sec, 87 tokens
âœ… Session 2 completed: 48.92 tokens/sec, 95 tokens

============================================================
ğŸ“Š PARALLEL EVALUATION RESULTS
============================================================
ğŸ¯ Session 0:
   ğŸ“ Prompt: Write a short poem about machine learning...
   ğŸ“Š Performance: 45.23 tokens/sec
   ğŸ”¢ Tokens: 98
   â±ï¸  Duration: 2.17s
   ğŸ’¬ Generated: In circuits deep, where data flows like streams...

ğŸ“ˆ Average performance: 45.43 tokens/sec across 3 sessions
â±ï¸  Total time: 2.45s

ğŸ”’ Thread safety maintained through:
   â€¢ Pre-evaluated model weights
   â€¢ Task-local random states
   â€¢ Session-isolated KV caches
   â€¢ GPU operation serialization
```

## ğŸƒâ€â™‚ï¸ Running the Example

### In LLMEval App
1. Open the LLMEval app
2. Click **"Run Parallel Demo"** button
3. Watch the console output for parallel execution results

### Programmatic Usage
```swift
// Simple demo
await demonstrateParallelEvaluation()

// Advanced configuration
await runAdvancedParallelEvaluation()
```

## ğŸ”’ Thread Safety Verification

The example includes comprehensive thread safety measures:

- **Model Weight Evaluation**: Forces evaluation of all model parameters before concurrent use
- **Random State Isolation**: Each session uses `withRandomState()` with unique seeds
- **KV Cache Separation**: Each session maintains its own KV cache state
- **GPU Memory Limits**: Configurable cache limits prevent memory issues

## ğŸ’¡ Best Practices Demonstrated

1. **Pre-evaluate Models**: Always call `eval()` on models before concurrent use
2. **Use Task-Local Random State**: Employ `withRandomState()` for concurrent random operations
3. **Isolate Sessions**: Give each evaluation session its own resources
4. **Monitor Performance**: Track tokens/second and resource usage
5. **Handle Errors Gracefully**: Use proper error handling in concurrent code

This example serves as a reference implementation for safely running multiple LLM sessions concurrently in production applications.
